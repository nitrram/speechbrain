# Generated 2021-12-07 from:
# /home/martin/Projects/speechbrain/recipes/CommonVoice/ASR/transducer/hparams/train_cs.yaml
# yamllint disable
# ############################################################################
# Model: E2E ASR with attention-based ASR
# Encoder: CRDNN model
# Decoder: GRU + beamsearch + RNNLM
# Tokens: BPE with unigram
# losses: Transducer
# Training: CommonVoice CS
# Authors:  Abdel HEBA, Mirco Ravanelli, Sung-Lin Yeh 2020
# ############################################################################

# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1319
__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]
output_dir: !ref results/cv_transducer/<seed>
wer_file: !ref <output_dir>/wer.txt
save_dir: !ref <output_dir>/save
train_log: !ref <output_dir>/train_log.txt

# Data files
data_dir: dataset/cv-corpus-7.0-2021-07-21/cs # !PLACEHOLDER  # e.g, /localscratch/cv-corpus-5.1-2020-06-22/fr
train_tsv_file: !ref <data_dir>/train.tsv # Standard CommonVoice .tsv files
dev_tsv_file: !ref <data_dir>/dev.tsv # Standard CommonVoice .tsv files
test_tsv_file: !ref <data_dir>/test.tsv # Standard CommonVoice .tsv files
accented_letters: True
language: cs # use 'it' for Italian, 'rw' for Kinyarwanda, 'en' for english
train_csv: !ref <save_dir>/train.csv
valid_csv: !ref <save_dir>/dev.csv
test_csv: !ref <save_dir>/test.csv
skip_prep: False # Skip data preparation

# We remove utterance slonger than 10s in the train/dev/test sets as
# longer sentences certainly correspond to "open microphones".
avoid_if_longer_than: 10.0

# Training parameters
number_of_epochs: 30
batch_size: 6
batch_size_valid: 1
lr: 1.0
sorting: ascending
ckpt_interval_minutes: 15 # save checkpoint every N min
# MTL for encoder with CTC (uncomment enc_lin layer)
#number_of_ctc_epochs: 2
#ctc_weight: 0.33
# MTL for decoder with CE (uncomment dec_lin layer)
#number_of_ce_epochs: 2
#ce_weight: 0.33

# Feature parameters
sample_rate: 16000
n_fft: 400
n_mels: 80

opt_class: !name:torch.optim.Adadelta
  lr: !ref <lr>
  rho: 0.95
  eps: 1.e-8

# BPE parameters
token_type: char # ["unigram", "bpe", "char"]
character_coverage: 1.0

# Dataloader options
train_dataloader_opts:
  batch_size: !ref <batch_size>

valid_dataloader_opts:
  batch_size: !ref <batch_size_valid>

test_dataloader_opts:
  batch_size: 1

# Model parameters
activation: &id001 !name:torch.nn.LeakyReLU
dropout: 0.15
cnn_blocks: 3
cnn_channels: (128, 200, 256)
inter_layer_pooling_size: (2, 2, 2)
cnn_kernelsize: (3, 3)
time_pooling_size: 4
rnn_class: &id002 !name:speechbrain.nnet.RNN.LSTM
rnn_layers: 5
rnn_neurons: 1024
rnn_bidirectional: True
dnn_blocks: 2
dnn_neurons: 1024
dec_neurons: 1024
output_neurons: 50  # index(blank/eos/bos) = 0
joint_dim: 1024
blank_index: 0

# Decoding parameters
beam_size: 4
nbest: 1
# by default {state,expand}_beam = 2.3 as mention in paper
# https://arxiv.org/abs/1904.02619
state_beam: 2.3
expand_beam: 2.3

epoch_counter: &id012 !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: !ref <number_of_epochs>

normalize: &id008 !new:speechbrain.processing.features.InputNormalization
  norm_type: global

compute_features: !new:speechbrain.lobes.features.Fbank
  sample_rate: !ref <sample_rate>
  n_fft: !ref <n_fft>
  n_mels: !ref <n_mels>

# Frequency domain SpecAugment
augmentation: &id009 !new:speechbrain.lobes.augment.SpecAugment

# for MTL
# update model if any HEAD module is added
  time_warp: True
  time_warp_window: 5
  time_warp_mode: bicubic
  freq_mask: True
  n_freq_mask: 2
  time_mask: True
  n_time_mask: 2
  replace_with_zero: False
  freq_mask_width: 30
  time_mask_width: 40

enc: &id003 !new:speechbrain.lobes.models.CRDNN.CRDNN
  input_shape: [null, null, !ref <n_mels>]
  activation: *id001
  dropout: !ref <dropout>
  cnn_blocks: !ref <cnn_blocks>
  cnn_channels: !ref <cnn_channels>
  cnn_kernelsize: !ref <cnn_kernelsize>
  inter_layer_pooling_size: !ref <inter_layer_pooling_size>
  time_pooling: True
  using_2d_pooling: False
  time_pooling_size: !ref <time_pooling_size>
  rnn_class: *id002
  rnn_layers: !ref <rnn_layers>
  rnn_neurons: !ref <rnn_neurons>
  rnn_bidirectional: !ref <rnn_bidirectional>
  rnn_re_init: True
  dnn_blocks: !ref <dnn_blocks>
  dnn_neurons: !ref <dnn_neurons>

# For MTL CTC over the encoder
# enc_lin: !new:speechbrain.nnet.linear.Linear
#     input_size: !ref <dnn_neurons>
#     n_neurons: !ref <joint_dim>
#
# ctc_cost: !name:speechbrain.nnet.ctc_loss
#    blank_index: !ref <blank_index>

emb: &id004 !new:speechbrain.nnet.embedding.Embedding
  num_embeddings: !ref <output_neurons>
  consider_as_one_hot: True
  blank_id: !ref <blank_index>

dec: &id005 !new:speechbrain.nnet.RNN.GRU
  input_shape: [null, null, !ref <output_neurons> - 1]
  hidden_size: !ref <dec_neurons>
  num_layers: 1
  re_init: True

# For MTL with LM over the decoder
# dec_lin: !new:speechbrain.nnet.linear.Linear
#     input_size: !ref <dec_neurons>
#     n_neurons: !ref <joint_dim>
#     bias: False
#
# ce_cost: !name:speechbrain.nnet.nll_loss
#    label_smoothing: 0.1

Tjoint: &id006 !new:speechbrain.nnet.transducer.transducer_joint.Transducer_joint
  joint: sum  # joint [sum | concat]
  nonlinearity: *id001
  
transducer_lin: &id007 !new:speechbrain.nnet.linear.Linear
  input_size: !ref <joint_dim>
  n_neurons: !ref <output_neurons>
  bias: False

log_softmax: !new:speechbrain.nnet.activations.Softmax
  apply_log: True

transducer_cost: !name:speechbrain.nnet.losses.transducer_loss
  blank_index: !ref <blank_index>

# for MTL
# update model if any HEAD module is added
modules:
  enc: *id003
  emb: *id004
  dec: *id005
  Tjoint: *id006
  transducer_lin: *id007
  normalize: *id008
  augmentation: *id009

model: &id010 !new:torch.nn.ModuleList
- [*id003, *id004, *id005, *id007]

greedy_searcher: !new:speechbrain.decoders.transducer.TransducerBeamSearcher
  decode_network_lst: [*id004, *id005]
  tjoint: *id006
  classifier_network: [*id007]
  blank_id: !ref <blank_index>
  beam_size: 1
  nbest: !ref <nbest>

beam_searcher: !new:speechbrain.decoders.transducer.TransducerBeamSearcher
  decode_network_lst: [*id004, *id005]
  tjoint: *id006
  classifier_network: [*id007]
  blank_id: !ref <blank_index>
  beam_size: !ref <beam_size>
  nbest: !ref <nbest>
  state_beam: !ref <state_beam>
  expand_beam: !ref <expand_beam>

lr_annealing: &id011 !new:speechbrain.nnet.schedulers.NewBobScheduler
  initial_value: !ref <lr>
  improvement_threshold: 0.0025
  annealing_factor: 0.8
  patient: 0

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <save_dir>
  recoverables:
    model: *id010
    scheduler: *id011
    normalize: *id008
    counter: *id012
    
train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: !ref <train_log>

error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats

cer_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats
  split_tokens: True

# decoder's
#seq_lin: !new:speechbrain.nnet.linear.Linear
#    input_size: !ref <dec_neurons>
#    n_neurons: !ref <output_neurons>

#tokenizer_file: asr-rnnt-commonvoice-cs/1000_unigram.model
#tokenizer: &id013 !new:sentencepiece.SentencePieceProcessor

#encoder: !new:speechbrain.nnet.containers.LengthsCapableSequential
#    input_shape: [null, null, !ref <n_mels>]
#    compute_features: !ref <compute_features>
#    normalize: *id008
#    model: !ref <enc>

#modules:
#    normalize: *id008
#    encoder: !ref <encoder>
#    decoder: !ref <beam_searcher>
  
#pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
#  loadables:
#    tokenizer: *id013
#    normalize: *id008
#    model: *id010
#  paths:
#    tokenizer: !ref <tokenizer_file>
